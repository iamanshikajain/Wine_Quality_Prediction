# -*- coding: utf-8 -*-
"""Research Project on Wine Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K7_DYrMSnZrJ8nFl63aXLK9UP_Z3GPyg

## Importing the libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Importing the dataset"""

w_dataset = pd.read_csv('winequality-red.csv')

w_dataset.head()

w_dataset.shape

"""# Checking if there is any null values or not"""

w_dataset.isnull().sum()

"""## `Data Analysis`"""

w_dataset.describe()

"""# Checking the coorelation"""

c = w_dataset.corr()

#Graph visualizing the coorelation

plt.figure(figsize=(10,10))
sns.heatmap(c, cbar=True, square=True, xticklabels=True,yticklabels=True,fmt = '.1f', annot = True, annot_kws={'size':8}, cmap = 'YlGnBu')

"""## Data Visualization"""

#count
sns.catplot(x='quality', data = w_dataset, kind = 'count')

# volatile acidity vs Quality
plot = plt.figure(figsize=(5,5))
sns.barplot(x='quality', y = 'volatile acidity', data = w_dataset)

# citric acid vs Quality
plot = plt.figure(figsize=(5,5))
sns.barplot(x='quality', y = 'citric acid', data = w_dataset)

"""## Data Processing"""

X = w_dataset.drop('quality',axis=1)

Y = w_dataset['quality'].apply(lambda y_value: 1 if y_value>=7 else 0)

"""Splitting the dataset into train set and test set"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2,random_state=0)

"""### **Models**

### Logistic Regression

Applying feature scaling
"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_1 = sc.fit_transform(X_train)
X_test_1 = sc.transform(X_test)

from sklearn.linear_model import LogisticRegression
classifier_1 = LogisticRegression(random_state = 3)
classifier_1.fit(X_train_1, Y_train)

from sklearn.metrics import accuracy_score
X_test_prediction_1 = classifier_1.predict(X_test_1)
test_data_accuracy_1 = accuracy_score(X_test_prediction_1, Y_test)
print(test_data_accuracy_1)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier_1, X = X_test_1, y = Y_test, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))

"""### XG Boost"""

from xgboost import XGBClassifier
classifier_2 = XGBClassifier()
classifier_2.fit(X_train_1, Y_train)

from sklearn.metrics import accuracy_score
X_test_prediction_2 = classifier_2.predict(X_test_1)
test_data_accuracy_2 = accuracy_score(X_test_prediction_2, Y_test)
print(test_data_accuracy_2)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier_2, X = X_test_1, y = Y_test, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))

"""### Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
classifier_3 = RandomForestClassifier()
classifier_3.fit(X_train_1,Y_train)

from sklearn.metrics import accuracy_score
X_test_prediction_3 = classifier_3.predict(X_test_1)
test_data_accuracy_3 = accuracy_score(X_test_prediction_3, Y_test)
print(test_data_accuracy_3)

"""### SVC"""

from sklearn.svm import SVC
classifier_4 = SVC(kernel = 'linear', random_state = 0)
classifier_4.fit(X_train, Y_train)

from sklearn.metrics import accuracy_score
X_test_prediction_4 = classifier_4.predict(X_test)
test_data_accuracy_4 = accuracy_score(X_test_prediction_4, Y_test)
print(test_data_accuracy_4)

"""### Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
classifier_5 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier_5.fit(X_train, Y_train)

from sklearn.metrics import accuracy_score
X_test_prediction_5 = classifier_5.predict(X_test)
test_data_accuracy_5 = accuracy_score(X_test_prediction_5, Y_test)
print(test_data_accuracy_5)

"""### Knn"""

from sklearn.neighbors import KNeighborsClassifier
classifier_6 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier_6.fit(X_train_1, Y_train)

from sklearn.metrics import accuracy_score
X_test_prediction_6 = classifier_6.predict(X_test_1)
test_data_accuracy_6 = accuracy_score(X_test_prediction_6, Y_test)
print(test_data_accuracy_6)

"""### Gaussian NB"""

from sklearn.naive_bayes import GaussianNB
classifier_7 = GaussianNB()
classifier_7.fit(X_train, Y_train)

from sklearn.metrics import accuracy_score
X_train_prediction_7 = classifier_7.predict(X_train)
test_data_accuracy_7 = accuracy_score(X_train_prediction_7, Y_train)
print(test_data_accuracy_7)

"""### Graphically Comparing the accuracies"""

plt.figure(figsize=(14,10))
sns.barplot(y=[test_data_accuracy_1,test_data_accuracy_2,test_data_accuracy_3,test_data_accuracy_4,test_data_accuracy_5,test_data_accuracy_6,test_data_accuracy_7],x=['Logistic Regression','XG Boost','Random Forest Classifier','SVC','Knn','Decision Tree Classifier','Gaussian NB'])
plt.title('Model Accuracy',fontsize=20)
plt.xlabel('Accuracy',fontsize=15)
plt.ylabel('ML Models',fontsize=15)
plt.show()

"""### **Conclusion Random Forest Classifier is best for this project.**"""